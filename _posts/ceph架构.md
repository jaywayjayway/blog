title: 普通人扯ceph架构
categories: ceph
tags: [ceph]
date: 2016-05-28 21:53:09
---
今天在来扯点啥呢？平时由于吊丝运维的工作关系，杂七杂八地也搞了不少东西。偶然间邂逅了ceph,各种高大上的概念与理念就像是妹子身上的幽香（此处运用通感手法），吸引着让我继续探索（读者切毋遐想）。
什么是ceph呢？引述下官文结出的定义:  `Ceph is a unified, distributed storage system designed for excellent performance, reliability and scalability.`   鄙人英文水平有限，通过有道翻译，把几个关键字拎出来。
```
    unified   //统一的
    distributed  //分布的
```
官文就是给力，寥寥几句就能给人醍醐灌顶的赶脚。所谓的“统一的”就是指ceph同时可能向外提供对象存储、块存储、文件存储这三类存储功能。“分布式”是说ceph没有中心结构，可以平滑地水平扩展。
说得好高大啊（言外之意就是逼格好高），这几类存储是啥鸟玩样儿呢？鄙人再来小扯下。
<!--more-->

- **对象存储**

对象存储可不是把妹子拐来自个儿藏起来的意思。是基于对象的存储. 对象存储的基本单元是Object。每个Object是数据和数据属性集的综合体。对象到底长啥模样呢？来看看ceph官文的写真图

从写真图看，对象是一个扁平的存储空间，只包含了`ID`，`Binary Data`,`Metadata`三个组成部分，不存在普通文件系统那样的目录层级，所以索引速度会很快，一次索引就是找到它。用高大上的概念讲，就是那个啥算法复杂度比较低，貌似只有`log(n)`  （希望有大牛指正下  ^_^,让小弟也提升下档次）

-  **块存储**

在块存储中，块是一段标准长度（块大小）的字节或比特。在处理计算机程序产生的数据流时，块化数据可以简化处理过程。一般情况下，一次会读取一个完整的块—-wiki里的定义就是牛X。鄙人的个人理解，块存储就是说每个块有一个block size的最小单元，最直接的就是在linux系统里能够用 blkid 查看到。

-  **文件存储**

文件存储通过特殊的协议比如 `NFS`  or `SMB/CIFS` 来实现目录及文件的存储和访问。文件及目录有层级关系。下面我们也来看看它的真身。

根据个人的理解就算是扯完了ceph的统一存储，是不是觉得它很有魅力呢。来张逼格高点的图吧，增加印像。

此图可谓分布精巧，架构优美，可谓旷世奇作….（忍不住要品头论足了，失敬失敬）。ceph的核心就是这个**RADOS**的东西，真是母牛坐蒸笼啊！！！自我修复，自我管理，机器人一般。关于RADOS，有兴趣的同志可以好好拜读下创始人的 论文 。 等待同道中人的分享。（普通小弟在此先叩谢了）。下面就对这张高规格的图做下解析，以下解析内容来源  “一棹凌烟 且行且悟“的 《[“Ceph浅析”系列之四——Ceph的结构](http://yizhaolingyan.net/?p=55)》 ，在此向大神拜谢，如果侵害到大神的著作权，小人立即焚之。
自下向上，可以将Ceph系统分为四个层次：
-（1）基础存储系统RADOS
         顾名思义，这一层本身就是一个完整的对象存储系统，所有存储在Ceph系统中的用户数据事实上最终都是由这一层来存储的。而Ceph的高可靠、高可扩展、高性能、高自动化等等特性本质上也是由这一层所提供的。因此，理解RADOS是理解Ceph的基础与关键。         物理上，RADOS由大量的存储设备节点组层，每个节点拥有自己的硬件资源（CPU、内存、硬盘、网络），并运行着操作系统和文件系统。4.2、4.3节将对RADOS进行展开介绍。
         
-（2）基础库librados
这一层的功能是对RADOS进行抽象和封装，并向上层提供API，以便直接基于RADOS（而不是整个Ceph）进行应用开发。特别要注意的是，RADOS是一个对象存储系统，因此，librados实现的API也只是针对对象存储功能的。
RADOS采用C++开发，所提供的原生librados API包括C和C++两种，其文档参见[ 2 ]。物理上，librados和基于其上开发的应用位于同一台机器，因而也被称为本地API。应用调用本机上的librados API，再由后者通过socket与RADOS集群中的节点通信并完成各种操作。

-（3）高层应用接口
这一层包括了三个部分：RADOS GW（RADOS Gateway）、 RBD（Reliable Block Device）和Ceph FS（Ceph File System），其作用是在librados库的基础上提供抽象层次更高、更便于应用或客户端使用的上层接口。
其中，RADOS GW是一个提供与Amazon S3和Swift兼容的RESTful API的gateway，以供相应的对象存储应用开发使用。RADOS GW提供的API抽象层次更高，但功能则不如librados强大。因此，开发者应针对自己的需求选择使用。
RBD则提供了一个标准的块设备接口，常用于在虚拟化的场景下为虚拟机创建volume。目前，Red Hat已经将RBD驱动集成在KVM/QEMU中，以提高虚拟机访问性能。
Ceph FS是一个POSIX兼容的分布式文件系统。由于还处在开发状态，因而Ceph官网并不推荐将其用于生产环境中。
-（4）应用层
这一层就是不同场景下对于Ceph各个应用接口的各种应用方式，例如基于librados直接开发的对象存储应用，基于RADOS GW开发的对象存储应用，基于RBD实现的云硬盘等等。
在上文的介绍中，有一个地方可能容易引起困惑：RADOS自身既然已经是一个对象存储系统，并且也可以提供librados API，为何还要再单独开发一个RADOS GW？
话说，扯到这儿，ceph是个什么具体实物，毛都没看见，有读者是不是想要K我了，哈哈！效果达到了，就是要这样的效果，我的地盘我作主。下面开始扯扯ceph的身体构造。是不是凹凸有致，别有韵味呢？ 
我大学的马老师对我影响最深的一句话就是“一图胜千言”，稀里哗啦码了不少文字，还是来一张画龙点睛的图最佳。学生时期没跟马老师好好学画图，现在只能做个盗图党了。在此奉劝还是学生党的同志们，别光顾着风花雪月，掌握一技之长，方能屹立不倒。（太说教了，自己都想抽自己了）。言归正传，讲ceph的组成。

    * osd

osd全称Object Storage Device,是用来实现数据存储与维护的。具体到系统上，osd包含两部分，即存储介质和守护进程。通常情况下，一个osd 对应一块硬盘（存储介质）和一个ceph-osd 守护进程。

    * monitor

monitor从字面意义讲就是“监视器”，如果在交流过程中一直用“监视器”来指代monitor,那显得档次多Low啊。因此，为了显示自己有档次（打肿脸充个胖子），必须用mon来表示。mon的作用跟监视器差不多（老外挺能想），它掌握了整个ceph集群的状态，在ceph里的状态最直接的表现就是各种map，包括 monr map, t OSD map, the Placement Group (PG) map,  CRUSH map。

    * MDS

mds即metadata Server,元数据服务主要是用来实现CephFS文件级存储的，对象存储、块存储并不需要。通过MDS，client就可以像普通文件系统那样使用ls,find等命令了。还是来张图帮助理解吧。

粗糙的理解，MDS就是用来构造目录结构的，当然其本身也是分布式。具体细节鄙人也不是能很明白的扯上来，找google大神求助吧。(^_^,不轻易间暴露了自己很Low)
先扯到这儿吧，致止敬礼！ 期待下次吧，，，哈哈！！！

